{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cd3c68",
   "metadata": {},
   "source": [
    "Got it! You want to **build a Sentiment Analysis Classifier** using the mock data you generated. Here's how to do it **step by step**, with **deliverables covered**:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Task Plan (How to Complete Your Project)\n",
    "\n",
    "### **1. Generate Mock Data**\n",
    "\n",
    "You already have:\n",
    "\n",
    "```python\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "reviews = [fake.text(max_nb_chars=200) for _ in range(5000)]\n",
    "labels = [random.choice(['positive', 'neutral', 'negative']) for _ in range(5000)]\n",
    "```\n",
    "\n",
    "Convert to a **DataFrame**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'review': reviews, 'sentiment': labels})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Preprocessing Pipeline**\n",
    "\n",
    "Your preprocessing steps should include:\n",
    "\n",
    "âœ” Lowercasing\n",
    "âœ” Remove punctuation\n",
    "âœ” Remove stopwords\n",
    "âœ” Tokenization\n",
    "âœ” (Optional) Lemmatization/Stemming\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()                                # Lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)                       # Tokenize\n",
    "    tokens = [w for w in tokens if w not in stop_words] # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_review'] = df['review'].apply(preprocess)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Train/Test Split**\n",
    "\n",
    "Split data into **training and testing**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['clean_review']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Feature Extraction**\n",
    "\n",
    "Convert text into **numeric features** using TF-IDF:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Model Training**\n",
    "\n",
    "#### Option A: **Naive Bayes**\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "```\n",
    "\n",
    "#### Option B: **LSTM or BERT**\n",
    "\n",
    "* **LSTM**: Requires converting text into sequences (using `Tokenizer` + `Embedding`).\n",
    "* **BERT**: Use HuggingFace Transformers for pre-trained embeddings.\n",
    "  *(If you want code for these, tell me!)*\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Evaluation**\n",
    "\n",
    "Compute metrics and **confusion matrix**:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positive','neutral','negative'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['positive','neutral','negative'], yticklabels=['positive','neutral','negative'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Word Cloud Visualization**\n",
    "\n",
    "Create **word clouds** for each sentiment:\n",
    "\n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    text = ' '.join(df[df['sentiment'] == sentiment]['clean_review'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{sentiment.capitalize()} Reviews Word Cloud')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Deliverables Completed**\n",
    "\n",
    "âœ” Preprocessing pipeline\n",
    "âœ” Train/Test split\n",
    "âœ” Model training (Naive Bayes)\n",
    "âœ” Evaluation (Accuracy, Confusion Matrix)\n",
    "âœ” Visualization (Word Clouds)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "Do you want me to:\n",
    "âœ… **Write the full working code (all steps together)** for **Naive Bayes**, OR\n",
    "âœ… Add **LSTM version**, OR\n",
    "âœ… Add **BERT version** with HuggingFace?\n",
    "\n",
    "ðŸ‘‰ Which one do you want first? **Naive Bayes (quick), LSTM, or BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739447c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
